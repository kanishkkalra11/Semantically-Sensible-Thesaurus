{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMKdV5X9yt4W"
   },
   "source": [
    "Loading and Preprocessing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "XYWDnbQZ_9zx",
    "outputId": "20fda263-2d52-45b6-aa75-35760957f816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/91/967c3bc6c4601fa9d36043f580c3ad691722ed82a71c57016ecbb48a088c/tensorly-0.4.4.tar.gz (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.3.1)\n",
      "Collecting nose\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 14.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: tensorly\n",
      "  Building wheel for tensorly (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorly: filename=tensorly-0.4.4-cp36-none-any.whl size=98403 sha256=dbe97a767b8d8b18a76a15843e8bf72c7f1aa3a31c22308f9745f00243f1004c\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/2a/e7/a8efd4828f2b83227355c943cce62bf404d1eb07ce5f081181\n",
      "Successfully built tensorly\n",
      "Installing collected packages: nose, tensorly\n",
      "Successfully installed nose-1.3.7 tensorly-0.4.4\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "\n",
    "import nltk, math, re\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install tensorly\n",
    "import tensorly as tl\n",
    "from scipy import spatial\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prqTABrx_21U"
   },
   "outputs": [],
   "source": [
    "# Loading Dataset: BNC-Baby\n",
    "\n",
    "bnc = nltk.corpus.reader.bnc.BNCCorpusReader(root='Datasets/BNC_baby/Texts/', fileids=r'.*.xml')\n",
    "sentences = bnc.tagged_sents(c5=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WswbzL3f91A8"
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for i in range(len(sentences)):\n",
    "    sents.append([])\n",
    "    for j in range(len(sentences[i])):\n",
    "        a = list(sentences[i][j])\n",
    "        a[0] = a[0].lower()\n",
    "        a[0] = lemmatizer.lemmatize(a[0])\n",
    "        sents[i].append(tuple(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN3ViVz0A36q"
   },
   "outputs": [],
   "source": [
    "# Extracting data in sentences\n",
    "\n",
    "al_tag = (\n",
    "    'AJ0','AJC','AJS','AV0','AVP','AVQ','CJS','DPS','DTQ','EX0','NN0','NN1','NN2','NP0','PNI','PNP','PNQ','PNX',\n",
    "    'VBB','VBD','VBG','VBI','VBN','VBZ','VDB','VDD','VDI','VDG','VDN','VDZ','VHB','VHD','VHG','VHI','VHN','VHZ',\n",
    "    'VM0','VVB','VVD','VVG','VVI','VVN','VVZ'\n",
    ") #all tags that we want to keep\n",
    "\n",
    "nountags =  ('NN0','NN1','NN2','NP0','PNI','PNP','PNQ','PNX')\n",
    "\n",
    "verbtags =  ('VBB','VBD','VBG','VBI','VBN','VBZ','VDB','VDD','VDI','VDG','VDN','VDZ','VHB','VHD','VHG','VHI','VHN','VHZ',\n",
    "    'VM0','VVB','VVD','VVG','VVI','VVN','VVZ')\n",
    "\n",
    "tokens = 0\n",
    "sentslist = []\n",
    "nounslist = []\n",
    "verbslist = []\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    a = ''\n",
    "    for j in range(len(sents[i])):\n",
    "        # if(sents[i][j][1] in al_tag):\n",
    "            tokens += 1\n",
    "            a = a + sents[i][j][0]\n",
    "            a = a + ' '\n",
    "    #     if(sents[i][j][1] in nountags):\n",
    "    #         nounslist.append(sents[i][j][0])\n",
    "    #     if(sents[i][j][1] in verbtags):\n",
    "    #         verbslist.append(sents[i][j][0])\n",
    "    if (a != ''):\n",
    "        sentslist.append(a[:-1])\n",
    "    else:\n",
    "        del sentslist[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uai98QKMGRG8",
    "outputId": "bdae1545-aa25-41c8-cd17-e382b30d058c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600407\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJMSZDF06_R6"
   },
   "outputs": [],
   "source": [
    "with open('/TensorBasedFactorizationModelUtilities/listofBNCbabysentences.txt', 'w') as f:\n",
    "    for item in sentslist:\n",
    "        f.write(\"%s\\n\" %item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwHSvLKBF2r1"
   },
   "outputs": [],
   "source": [
    "# calculating frequencies of nouns and verbs\n",
    "\n",
    "nounfreq = sorted(FreqDist(nounslist).items(),key=lambda k:k[1], reverse=True)\n",
    "verbfreq = sorted(FreqDist(verbslist).items(),key=lambda k:k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLAa3ulLkDav"
   },
   "outputs": [],
   "source": [
    "with open('/TensorBasedFactorizationModelUtilities/Nounsfrequency_BNCbaby.txt', 'w') as f:\n",
    "    for item in nounfreq:\n",
    "        f.write(\"%s\\n\" %str(item))\n",
    "\n",
    "with open('/TensorBasedFactorizationModelUtilities/Verbsfreq_BNCbaby.txt', 'w') as f:\n",
    "    for item in verbfreq:\n",
    "        f.write(\"%s\\n\" %str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FNCHvpEQ3tj"
   },
   "outputs": [],
   "source": [
    "# creating a dictionary of context words for all nouns\n",
    "\n",
    "contextwordsdict = {}\n",
    "for i in range(len(sents)):\n",
    "    for j in range(len(sents[i])):\n",
    "        if(sents[i][j][1] in nountags):\n",
    "            if(sents[i][j][0] not in contextwordsdict.keys()):\n",
    "                contextwordsdict[sents[i][j][0]] = {}\n",
    "            for k in range(len(sents[i])):\n",
    "                if ((k!=j) and (sents[i][k][1] in al_tag)):\n",
    "                    try:\n",
    "                        contextwordsdict[sents[i][j][0]][sents[i][k][0]] += 1\n",
    "                    except:\n",
    "                        contextwordsdict[sents[i][j][0]][sents[i][k][0]] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6C7YwHjkKgQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json = json.dumps(contextwordsdict)\n",
    "f = open(\"/TensorBasedFactorizationModelUtilities/Nouns-Contextwordsdict.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_I1YuvhMlGUH"
   },
   "outputs": [],
   "source": [
    "# calculating frequency of context words\n",
    "\n",
    "contextwordslist = []\n",
    "for item in contextwordsdict.keys():\n",
    "    for context in contextwordsdict[item].keys():\n",
    "        for i in range(contextwordsdict[item][context]):\n",
    "            contextwordslist.append(context)\n",
    "contextfreq = sorted(FreqDist(contextwordslist).items(),key=lambda k:k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qHKVBKia15p"
   },
   "source": [
    "Calculating Latent Factors of Nouns using NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_xu-dq7woI7"
   },
   "outputs": [],
   "source": [
    "# Generating the Nouns-Context Words Matrix W considering 100 most frequent nouns and 3000 most frequent context words, weighted using PMI\n",
    "\n",
    "W = np.zeros([1000, 3000])\n",
    "for i in range(1000):\n",
    "    p1 = float(nounfreq[i][1])/tokens\n",
    "    for j in range(3000):\n",
    "        p2 = float(contextfreq[j][1])/tokens\n",
    "        try:\n",
    "            p_joint = float(contextwordsdict[nounfreq[i][0]][contextfreq[j][0]])/tokens\n",
    "            p = p_joint/(p1*p2)\n",
    "            W[i][j] = math.log(p,2)\n",
    "            if (W[i][j] < 0):\n",
    "                W[i][j] = 0\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GkHHDNbqkPe"
   },
   "outputs": [],
   "source": [
    "np.save('/TensorBasedFactorizationModelUtilities/Nouns-ContextWordsMatrix',W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ok5Uo2J7u5Qx"
   },
   "outputs": [],
   "source": [
    "# NMF\n",
    "\n",
    "model = NMF(n_components=300, init='random', random_state=0, beta_loss='kullback-leibler', solver='mu', max_iter=1000)\n",
    "W = model.fit_transform(W) # Noun Latent Factors\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0sufz22BcpE"
   },
   "outputs": [],
   "source": [
    "np.save('/TensorBasedFactorizationModelUtilities/NounLatentFactorsBNCBaby',W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GxiX7Sbbf61"
   },
   "source": [
    "Decomposition of subject-verb-object tensor using noun latent factors to obtain a core tensor of verbs that models semantic compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiVVIELAJxMc"
   },
   "outputs": [],
   "source": [
    "nounfreq = []\n",
    "i = 0\n",
    "with open('/TensorBasedFactorizationModelUtilities/Nounsfrequency_BNCbaby.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        i += 1\n",
    "        l = line.split(',')\n",
    "        a = [str(l[0][2:-1]) , int(l[1][1:-2])]\n",
    "        nounfreq.append(tuple(a))\n",
    "        if (i==1000):\n",
    "            break\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1K42D3wLeO5"
   },
   "outputs": [],
   "source": [
    "verbfreq = []\n",
    "i = 0\n",
    "with open('/TensorBasedFactorizationModelUtilities/Verbsfreq_BNCbaby.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        i += 1\n",
    "        l = line.split(',')\n",
    "        a = [str(l[0][2:-1]) , int(l[1][1:-2])]\n",
    "        verbfreq.append(tuple(a))\n",
    "        if (i==1000):\n",
    "            break\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRqrWPdZNimx"
   },
   "outputs": [],
   "source": [
    "sentslist = []\n",
    "with open('/TensorBasedFactorizationModelUtilities/listofBNCbabysentences.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        sentslist.append(line[:-1])\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzKtIUKxOHXP"
   },
   "outputs": [],
   "source": [
    "W = np.load('/TensorBasedFactorizationModelUtilities/NounLatentFactorsBNCBaby.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InaPT30I4TPV"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((1000,1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vT7Zf5zg-HFr"
   },
   "outputs": [],
   "source": [
    "def count(a,b,c,sentslist):\n",
    "    ans = 0\n",
    "    for i in range(len(sentslist)):\n",
    "        if (re.match(r'.*' + a + r'.*' + b + r'.*' + c + r'.*', sentslist[i]) is not None):\n",
    "            ans += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8914SzYV7pam"
   },
   "outputs": [],
   "source": [
    "# Creating SVO tensor considering 1000 most fequent nouns (sunjects and objects) and 1000 most frequent verbs, weighted using PMI\n",
    "\n",
    "for i in range(1000):\n",
    "    p1 = float(verbfreq[i][1])/tokens\n",
    "    for j in range(1000):\n",
    "        p2 = float(nounfreq[j][1])/tokens\n",
    "        for k in range(1000):\n",
    "            p3 = float(nounfreq[k][1])/tokens\n",
    "            p_joint = float(count(nounfreq[j][0],verbfreq[i][0],nounfreq[k][0],sentslist))/tokens\n",
    "            p = p_joint/(p1*p2*p3)\n",
    "            try:\n",
    "                X[i][j][k] = math.log(p,2)\n",
    "                if(X[i][j][k] < 0):\n",
    "                    X[i][j][k] = 0\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMA88NRzB-9r"
   },
   "outputs": [],
   "source": [
    "np.save('/TensorBasedFactorizationModelUtilities/SVOTensor_BNCBaby', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPH7uQ0oCQbB"
   },
   "outputs": [],
   "source": [
    "X_tensor = tf.convert_to_tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CihPNH6nCthI"
   },
   "outputs": [],
   "source": [
    "# Tensor Decomposition\n",
    "\n",
    "temp = tl.tenalg.mode_dot(X, W.T, 2)\n",
    "G = tl.tenalg.mode_dot(Y , W.T, 1) #core tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PSuiIYEC5jY"
   },
   "outputs": [],
   "source": [
    "np.save('/TensorBasedFactorizationModelUtilities/CoreTensor_BNCBaby',G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4yKHGNcgh0J"
   },
   "source": [
    "Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEt2nz9IDBRJ"
   },
   "outputs": [],
   "source": [
    "# Finding the composition matrix of verb v considering subject and object in context\n",
    "\n",
    "def svocompositioncontextual(a,b,c):\n",
    "    a_ind = -1\n",
    "    b_ind = -1\n",
    "    c_ind = -1\n",
    "    for i in range(len(nounfreq)):\n",
    "        if (nounfreq[i][0] == a):\n",
    "            a_ind = i\n",
    "        if (nounfreq[i][0] == c):\n",
    "            c_ind = i\n",
    "        if (a_ind!=-1 and c_ind!=-1):\n",
    "            break\n",
    "    for i in range(len(verbfreq)):\n",
    "        if (verbfreq[i][0] == b):\n",
    "            b_ind = i\n",
    "        if (b_ind!=-1):\n",
    "            break\n",
    "\n",
    "    if (a_ind==-1 or b_ind==-1 or c_ind==-1):\n",
    "        return -1 # if verb or subject or object not available in our training dataset\n",
    "\n",
    "    s = W[a_ind]\n",
    "    o = W[c_ind]\n",
    "    Y = np.outer(s,o) # vector outer product\n",
    "\n",
    "    Gv = G[b_ind]\n",
    "\n",
    "    Z = np.multiply(Gv,Y) # Hadamard product\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nh1BpcZhGLBA"
   },
   "outputs": [],
   "source": [
    "# finding the composition matric for a verb without considering context\n",
    "\n",
    "def svocompositionnoncontextual(b):\n",
    "    b_ind = -1\n",
    "    for i in range(len(verbfreq)):\n",
    "        if (verbfreq[i][0] == b):\n",
    "            b_ind = i\n",
    "        if (b_ind!=-1):\n",
    "            break\n",
    "    if (b_ind==-1):\n",
    "        return -1 # if verb not present in our training dataset\n",
    "\n",
    "    Z = G[b_ind] # slice of core tensor\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-GlHYw5GYgv"
   },
   "outputs": [],
   "source": [
    "# Calculating similarity between two matrices\n",
    "\n",
    "def similarity(A,B):\n",
    "    C = np.asmatrix(np.full((300,1),(float(1)/math.sqrt(300)))) # column vector to convert matrix into a vectorized and normalized representation\n",
    "    a = np.asarray(np.dot(A,C)) # matrix multiplication to obtain column vector\n",
    "    b = np.asarray(np.dot(B,C))\n",
    "    return (1 - spatial.distance.cosine(a,b)) # cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lY7IQ-3JTz-"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "ranks = []\n",
    "scores []\n",
    "\n",
    "fp = open('/Datasets/GS2011/GS2011data.txt', 'r') # Test Dataset\n",
    "line = fp.readline()\n",
    "line = fp.readline()\n",
    "while(line):\n",
    "    l = line.split()\n",
    "    A = svocompositioncontextual(l[2],l[1],l[3]) # contextual target word\n",
    "    try:\n",
    "        if (A==-1):\n",
    "            continue # ignoring if the test case does not exist in training data\n",
    "    except:\n",
    "        garbage = 0\n",
    "    B = svocompositionnoncontextual(l[4]) # non contextual landmark verb\n",
    "    try:\n",
    "        if (B==-1):\n",
    "            continue\n",
    "    except:\n",
    "        garbage = 0\n",
    "    scores.append(similarity(A,B))\n",
    "    ranks.append(int(line[5]))\n",
    "    line = fp.readline()\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkQNN-aeMf_9"
   },
   "outputs": [],
   "source": [
    "# Spearman correlation coefficient\n",
    "\n",
    "rho = spearmanr(ranks,scores)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhS3bbzQg_9l"
   },
   "source": [
    "Predicting most suitable verb replacement for a given svo triple from the verbs in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AferHvSwPva1"
   },
   "outputs": [],
   "source": [
    "# Verb Replacement Prediction\n",
    "\n",
    "s = \n",
    "v = \n",
    "o = \n",
    "verb = verbfreq[0][0]\n",
    "if(svocompositioncontextual(s,v,o)!=-1):\n",
    "    maximum = similarity(svocompositioncontextual(s,v,o),G[0]) # finding the verb for which the matrix similarity is maximum\n",
    "    for i in range(1,1000):\n",
    "        a = similarity(svocompositioncontextual(s,v,o),G[i])\n",
    "        if (a > maximum):\n",
    "            maximum = a\n",
    "            verb = verbfreq[i][0]\n",
    "    print(\"Most Suitable Verb Replacement is - \" + verb)\n",
    "else:\n",
    "    print(\"SVO not in dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1OpUZFuheM1"
   },
   "source": [
    "Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uo6vWu90hdwX"
   },
   "outputs": [],
   "source": [
    "# Our baseline is when we igonre the context of our target as well as landmark verbs\n",
    "\n",
    "ranks = []\n",
    "scores []\n",
    "\n",
    "fp = open('/Datasets/GS2011/GS2011data.txt', 'r')\n",
    "line = fp.readline()\n",
    "line = fp.readline()\n",
    "while(line):\n",
    "    l = line.split()\n",
    "    A = svocompositionnoncontextual(l[1]) # non contextual target word\n",
    "    if (A==-1):\n",
    "        continue # ignoring if the target verb does not exist in training data \n",
    "    B = svocompositionnoncontextual(l[4]) # non contextual landmark verb\n",
    "    if (B==-1):\n",
    "        continue\n",
    "    scores.append(similarity(A,B))\n",
    "    ranks.append(int(line[5]))\n",
    "    line = fp.readline()\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iOY_sauiE3v"
   },
   "outputs": [],
   "source": [
    "# Spearman correlation coefficient\n",
    "\n",
    "rho = spearmanr(ranks,scores)\n",
    "print(rho)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorBasedFactorizationModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
