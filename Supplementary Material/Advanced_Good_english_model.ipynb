{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipD8bhbsLwG1"
   },
   "outputs": [],
   "source": [
    "# Loading the SVO of any type\n",
    "\n",
    "import pickle as p\n",
    "with open('/Datasets/svo_en_large.dat', 'rb') as f:\n",
    "  (Sub_dict, Verb_dict, Obj_dict, svo_list) = p.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIoOWcHZn11L"
   },
   "outputs": [],
   "source": [
    "# Creating the dictionary structure for svo, sv, vo and subject, verb and object\n",
    "\n",
    "# Notes down the total tokens of each\n",
    "svo_total = 0\n",
    "sub_total = 0\n",
    "verb_total = 0\n",
    "obj_total = 0\n",
    "sv_total = 0\n",
    "vo_total = 0\n",
    "\n",
    "# Notes down the total types of each\n",
    "svo_key = len(svo_list.keys())\n",
    "sub_key = len(Sub_dict.keys())\n",
    "obj_key = len(Obj_dict.keys())\n",
    "verb_key = len(Verb_dict.keys())\n",
    "\n",
    "sv_dict = {}\n",
    "vo_dict = {}\n",
    "\n",
    "for i in svo_list:\n",
    "  svo_total+=svo_list[i]\n",
    "  try:\n",
    "    sv_dict[(i[0],i[1])]+=1\n",
    "  except KeyError:\n",
    "    sv_dict[(i[0],i[1])]=1\n",
    "  try:\n",
    "    vo_dict[(i[1],i[2])]+=1\n",
    "  except KeyError:\n",
    "    vo_dict[(i[1],i[2])]=1\n",
    "\n",
    "for s in Sub_dict:\n",
    "  sub_total += Sub_dict[s]\n",
    "for v in Verb_dict:\n",
    "  verb_total += Verb_dict[v]\n",
    "for o in Obj_dict:\n",
    "  obj_total += Obj_dict[o]\n",
    "\n",
    "sv_key = len(sv_dict.keys())\n",
    "vo_key = len(vo_dict.keys())\n",
    "\n",
    "for sv in sv_dict:\n",
    "  sv_total += sv_dict[sv]\n",
    "for vo in vo_dict:\n",
    "  vo_total += vo_dict[vo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnQh4GiTyFVZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "uZ0OGYXNyFNJ",
    "outputId": "8b494f44-7ca7-4d6c-c8ff-bf2e7d0cc6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYcUAN_YyFEb"
   },
   "outputs": [],
   "source": [
    "bnc = nltk.corpus.reader.bnc.BNCCorpusReader(root='Datasets/BNC_baby/Texts/', fileids=r'.*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Qs83V9SymYk"
   },
   "outputs": [],
   "source": [
    "# Loading NLTK lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUgWTM0ey4sA"
   },
   "outputs": [],
   "source": [
    "noun_tag = ('nn0','nn1','nn2','np0','pni','pnp','pnq','pnx')\n",
    "verb_tag = ('vbb','vbd','vbg','vbi','vbn','vbz','vdb','vdd',\n",
    "            'vdi','vdg','vdn','vdz','vhb','vhd','vhg','vhi',\n",
    "            'vhn','vhz','vm0','vvb','vvd','vvg','vvi','vvn','vvz')\n",
    "\n",
    "noun_set = {}\n",
    "verb_set = {}\n",
    "count_obj = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOZEvlrXy4ju"
   },
   "outputs": [],
   "source": [
    "for sent in bnc.tagged_sents(c5=True):\n",
    "  for wt in sent:\n",
    "    try:\n",
    "      word=lemmatizer.lemmatize(wt[0].lower())\n",
    "      tag = wt[1].lower()\n",
    "    except:\n",
    "      continue\n",
    "    if tag in noun_tag:\n",
    "      try:\n",
    "        noun_set[word]+=1\n",
    "      except:\n",
    "        noun_set[word]=1\n",
    "    if tag in verb_tag:\n",
    "      try:\n",
    "        verb_set[word]+=1\n",
    "      except:\n",
    "        verb_set[word]=1\n",
    "    try:\n",
    "      count_obj[word]+=1\n",
    "    except:\n",
    "      count_obj[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RN6uiaifOsZ3"
   },
   "outputs": [],
   "source": [
    "verb_list = [j[1] for j in sorted(list(zip(verb_set.values(),verb_set.keys())), reverse=True)][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGRPV3o3OsMw"
   },
   "outputs": [],
   "source": [
    "noun_list = [j[1] for j in sorted(list(zip(noun_set.values(),noun_set.keys())), reverse=True)][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GV3UJEOoymPQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HWSHMgdymIN"
   },
   "outputs": [],
   "source": [
    "SYN_PARAM = 1./20\n",
    "HYP_PARAM = 1./40\n",
    "HYPO_PARAM = 1./40\n",
    "\n",
    "SVO_PARAM = 0.6\n",
    "SVtO_PARAM = 0.7\n",
    "SV_PARAM = 0.4\n",
    "VO_PARAM = 0.4\n",
    "SVt_PARAM = 0.5\n",
    "VtO_PARAM = 0.5\n",
    "\n",
    "SMOOTHING = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbEwiv1MyE4e"
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['R'] = np.array([SYN_PARAM, HYP_PARAM, HYPO_PARAM])\n",
    "parameters['F'] = np.array([SVO_PARAM, SVtO_PARAM, SV_PARAM, SVt_PARAM, VO_PARAM, VtO_PARAM])\n",
    "parameters['S'] = SMOOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpMb7z4CojpX"
   },
   "outputs": [],
   "source": [
    "# Creating a scoring object for easy access\n",
    "\n",
    "score_obj = {\n",
    "    'svo':svo_list,\n",
    "    'sub':Sub_dict,\n",
    "    'verb':Verb_dict,\n",
    "    'obj':Obj_dict,\n",
    "    'sv':sv_dict,\n",
    "    'vo':vo_dict,\n",
    "    'token_sv':sv_total,\n",
    "    'token_vo':vo_total,\n",
    "    'token_sub':sub_total,\n",
    "    'token_verb':verb_total,\n",
    "    'token_obj':obj_total,\n",
    "    'token_svo':svo_total,\n",
    "    'type_sub':sub_key,\n",
    "    'type_verb':verb_key,\n",
    "    'type_obj':obj_key,\n",
    "    'type_svo':svo_key,\n",
    "    'type_sv':sv_key,\n",
    "    'type_vo':vo_key,\n",
    "    'phi_svo':10,\n",
    "    'phi_sv':10,\n",
    "    'phi_vo':10,\n",
    "    'phi_sub':10,\n",
    "    'phi_verb':10,\n",
    "    'phi_obj':10,\n",
    "    'noun_list': noun_list,\n",
    "    'verb_list': verb_list,\n",
    "    'count': count_obj,\n",
    "    'parameters': parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVCHdS7O6Yy1"
   },
   "outputs": [],
   "source": [
    "score_obj['parameters']=parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALaTTQwQ5Z-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xU5128sCqMiG"
   },
   "outputs": [],
   "source": [
    "# Calculates the jaccard similarity\n",
    "def jaccard(s1, s2):\n",
    "  intersection = len(s1.intersection(s2))\n",
    "  union = len(s1.union(s2))\n",
    "  if union==0:\n",
    "    return 0.0\n",
    "  return  float(intersection)/float(union)\n",
    "\n",
    "# Calculates the R value between two words\n",
    "def get_R_word2word(w1, w2, parameters):\n",
    "  synonym1 = []\n",
    "  synonym2 = []\n",
    "  for ss in wordnet.synsets(w1):\n",
    "    synonym1.append(set(ss.lemma_names()))\n",
    "  for ss in wordnet.synsets(w2):\n",
    "    synonym2.append(set(ss.lemma_names()))\n",
    "  synonym_r = 0\n",
    "  for sen_1 in synonym1:\n",
    "    for sen_2 in synonym2:\n",
    "      synonym_r = max(synonym_r, jaccard(sen_1, sen_2))\n",
    "\n",
    "  hypernym1 = []\n",
    "  hypernym2 = []\n",
    "  for ss in wordnet.synsets(w1):\n",
    "    hypernym1.append(set(ss.hypernyms()))\n",
    "  for ss in wordnet.synsets(w2):\n",
    "    hypernym2.append(set(ss.hypernyms()))\n",
    "  hypernym_r = 0\n",
    "  for sen_1 in hypernym1:\n",
    "    for sen_2 in hypernym2:\n",
    "      hypernym_r = max(hypernym_r, jaccard(sen_1, sen_2))\n",
    "\n",
    "  hyponym1 = []\n",
    "  hyponym2 = []\n",
    "  for ss in wordnet.synsets(w1):\n",
    "    hyponym1.append(set(ss.hyponyms()))\n",
    "  for ss in wordnet.synsets(w2):\n",
    "    hyponym2.append(set(ss.hyponyms()))\n",
    "  hyponym_r = 0\n",
    "  for sen_1 in hyponym1:\n",
    "    for sen_2 in hyponym2:\n",
    "      hyponym_r = max(hyponym_r, jaccard(sen_1, sen_2))\n",
    "\n",
    "  # To be completed using senses\n",
    "  value = np.dot(np.array([synonym_r, hyponym_r, hypernym_r]), parameters)\n",
    "  return value\n",
    "\n",
    "# Calculates the R value between two phrases\n",
    "# Use get_R_word2word as base in this\n",
    "def get_R_p2w(p, w, parameters, tag, lemmatizer, count_obj):\n",
    "  tokens = [lemmatizer.lemmatize(j[0].lower()) for j in p.split()]\n",
    "  #wpos = nltk.pos_tag(' '.join(tokens))\n",
    "  #w_list = [j[0] for j in wpos if j[1][0]==tag]\n",
    "  R_l = []\n",
    "  weights = []\n",
    "  for word in tokens:\n",
    "    if word in count_obj:\n",
    "      R_l.append(get_R_word2word(word, w, parameters))\n",
    "      weights.append(1.0/count_obj['word'])\n",
    "  try:\n",
    "    return np.average(R_l, weights=weights)\n",
    "  except:\n",
    "    return 0\n",
    "  #word = lemmatizer.lemmatize(p.split()[-1][0].lower())\n",
    "  #return get_R_word2word(word, w, parameters)\n",
    "\n",
    "\n",
    "#w can be a svo, sv, vo\n",
    "#Target refers to the index of change\n",
    "#word_list get the list of words with \n",
    "#which we need to replace target position word\n",
    "#w_dict is dictionary of counts of existing refernce type\n",
    "def get_R_values_w(word_list, w_dict, w, target, tag, parameters, lemmatizer, count_obj):\n",
    "  R_w = 0\n",
    "  w_mut = list(w)\n",
    "  for word in word_list:\n",
    "    w_mut[target] = word\n",
    "    w_c = tuple(w_mut)\n",
    "    R_w+= get_R_p2w(w[target], word, parameters, tag, lemmatizer, count_obj)\n",
    "  return R_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za-MTAKa9pvE"
   },
   "outputs": [],
   "source": [
    "# Probability function for svo, sv, vo\n",
    "def get_prob_svo(svo, score_obj, lemmatizer):\n",
    "  R_svo = get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 0, 'N', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "  R_svo += get_R_values_w(score_obj['verb_list'], score_obj['svo'], svo, 1, 'V', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "  R_svo += get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 2, 'N', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  if svo in score_obj['svo']:\n",
    "    count = score_obj['svo'][svo]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_svo)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_svo'] + score_obj['token_svo'] + score_obj['phi_svo']\n",
    "  )\n",
    "\n",
    "  return prob\n",
    "\n",
    "def get_prob_sv(sv, score_obj, lemmatizer):\n",
    "  R_sv = get_R_values_w(score_obj['noun_list'], score_obj['sv'], sv, 0, 'N', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "  R_sv = get_R_values_w(score_obj['verb_list'], score_obj['sv'], sv, 1, 'V', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  if sv in score_obj['sv']:\n",
    "    count = score_obj['sv'][sv]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_sv)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_sv'] + score_obj['token_sv'] + score_obj['phi_sv']\n",
    "  )\n",
    "\n",
    "  return prob\n",
    "\n",
    "def get_prob_vo(vo, score_obj, lemmatizer):\n",
    "  R_vo = get_R_values_w(score_obj['verb_list'], score_obj['vo'], vo, 0, 'V', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "  R_vo += get_R_values_w(score_obj['noun_list'], score_obj['vo'], vo, 1, 'N', score_obj['parameters']['R'], lemmatizer, score_obj['count'])\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  if vo in score_obj['vo']:\n",
    "    count = score_obj['vo'][vo]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_vo)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_vo'] + score_obj['token_vo'] + score_obj['phi_vo']\n",
    "  )\n",
    "\n",
    "  return prob\n",
    "\n",
    "def get_prob_sub(sub, score_obj, lemmatizer):\n",
    "  R_s = 0\n",
    "  for word in score_obj['noun_list']:\n",
    "    R_s += get_R_p2w(sub, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "  \n",
    "  count = 0\n",
    "  if sub in score_obj['sub']:\n",
    "    count = score_obj['sub'][sub]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_s)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_sub'] + score_obj['token_sub'] + score_obj['phi_sub']\n",
    "  )\n",
    "\n",
    "  return prob\n",
    "\n",
    "def get_prob_verb(verb, score_obj, lemmatizer):\n",
    "  R_s = 0\n",
    "  for word in score_obj['verb_list']:\n",
    "    R_s += get_R_p2w(verb, word, score_obj['parameters']['R'], 'V', lemmatizer, score_obj['count'])\n",
    "  \n",
    "  count = 0\n",
    "  if verb in score_obj['verb']:\n",
    "    count = score_obj['verb'][verb]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_s)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_verb'] + score_obj['token_verb'] + score_obj['phi_verb']\n",
    "  )\n",
    "\n",
    "  return prob\n",
    "\n",
    "def get_prob_obj(obj, score_obj, lemmatizer):\n",
    "  R_s = 0\n",
    "  for word in score_obj['noun_list']:\n",
    "    R_s += get_R_p2w(obj, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "  \n",
    "  count = 0\n",
    "  if obj in score_obj['obj']:\n",
    "    count = score_obj['obj'][obj]\n",
    "  \n",
    "  prob = float(count + score_obj['parameters']['S'] + R_s)/(\n",
    "      score_obj['parameters']['S']*score_obj['type_obj'] + score_obj['token_obj'] + score_obj['phi_obj']\n",
    "  )\n",
    "\n",
    "  return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHNCxwmaB-WY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DSBMki_3pSZ"
   },
   "outputs": [],
   "source": [
    "#Getting the phi values for svo, sv, and vo\n",
    "#One time calculation. Updates for on the fly learning to be done separately\n",
    "#Need to do again if parameters for R calculation changed\n",
    "def set_phi_values(score_obj, lemmatizer):\n",
    "  param = score_obj['parameters']['R']\n",
    "  phi_svo = 0\n",
    "  phi_sv = 0\n",
    "  phi_vo = 0\n",
    "  phi_sub = 0\n",
    "  phi_verb = 0\n",
    "  phi_obj = 0\n",
    "  # Calculating for svo\n",
    "  for svo in score_obj['svo']:\n",
    "    #print(svo)\n",
    "    phi_svo+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 0,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_svo+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 2,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_svo+= get_R_values_w(score_obj['verb'], score_obj['svo'], svo, 1,'V', param, lemmatizer, score_obj['count'])\n",
    "  print('svo')\n",
    "  # Calculating for sv\n",
    "  for sv in score_obj['sv']:\n",
    "    phi_sv+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], sv, 0,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_sv+= get_R_values_w(score_obj['verb'], score_obj['svo'], sv, 1,'V', param, lemmatizer, score_obj['count'])\n",
    "  # Calculating for vo\n",
    "  for vo in score_obj['vo']:\n",
    "    phi_vo+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], vo, 1,'V', param, lemmatizer, score_obj['count'])\n",
    "    phi_vo+= get_R_values_w(score_obj['verb'], score_obj['svo'], vo, 0,'N', param, lemmatizer, score_obj['count'])\n",
    "  \n",
    "  # Calculating for sub\n",
    "  for sub in score_obj['sub']:\n",
    "    for word in score_obj['noun_list']:\n",
    "      phi_sub += get_R_p2w(sub, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "  \n",
    "  # Calculating for verb\n",
    "  for verb in score_obj['verb']:\n",
    "    for word in score_obj['verb_list']:\n",
    "      phi_verb += get_R_p2w(verb, word, score_obj['parameters']['R'], 'V', lemmatizer, score_obj['count'])\n",
    "\n",
    "  # Calculating for sub\n",
    "  for obj in score_obj['obj']:\n",
    "    for word in score_obj['noun_list']:\n",
    "      phi_obj += get_R_p2w(obj, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "\n",
    "  # Updating the parameters\n",
    "  score_obj['phi_svo'] = phi_svo\n",
    "  score_obj['phi_sv'] = phi_sv\n",
    "  score_obj['phi_vo'] = phi_vo\n",
    "  score_obj['phi_sub'] = phi_sub\n",
    "  score_obj['phi_verb'] = phi_verb\n",
    "  score_obj['phi_obj'] = phi_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oWGB21ojWqA"
   },
   "outputs": [],
   "source": [
    "#Getting the phi values for svo, sv, and vo\n",
    "#One time calculation. Updates for on the fly learning to be done separately\n",
    "#Need to do again if parameters for R calculation changed\n",
    "def set_phi_values_approx(score_obj, lemmatizer):\n",
    "  param = score_obj['parameters']['R']\n",
    "  phi_svo = 0\n",
    "  phi_sv = 0\n",
    "  phi_vo = 0\n",
    "  phi_sub = 0\n",
    "  phi_verb = 0\n",
    "  phi_obj = 0\n",
    "  # Calculating for svo\n",
    "  ind = np.random.choice(np.arange(score_obj['type_svo']), 30)\n",
    "  svo_ll = [list(score_obj['svo'].keys())[j] for j in ind]\n",
    "  for svo in svo_ll:\n",
    "    #print(svo)\n",
    "    phi_svo+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 0,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_svo+= get_R_values_w(score_obj['noun_list'], score_obj['svo'], svo, 2,'V', param, lemmatizer, score_obj['count'])\n",
    "    phi_svo+= get_R_values_w(score_obj['verb'], score_obj['svo'], svo, 1,'N', param, lemmatizer, score_obj['count'])\n",
    "  print('svo')\n",
    "  # Calculating for sv\n",
    "  ind = np.random.choice(np.arange(score_obj['type_sv']), 30)\n",
    "  svo_ll = [list(score_obj['sv'].keys())[j] for j in ind]\n",
    "  for sv in svo_ll:\n",
    "    phi_sv+= get_R_values_w(score_obj['noun_list'], score_obj['sv'], sv, 0,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_sv+= get_R_values_w(score_obj['verb'], score_obj['sv'], sv, 1,'V', param, lemmatizer, score_obj['count'])\n",
    "  # Calculating for vo\n",
    "  ind = np.random.choice(np.arange(score_obj['type_vo']), 30)\n",
    "  svo_ll = [list(score_obj['vo'].keys())[j] for j in ind]\n",
    "  for vo in svo_ll:\n",
    "    phi_vo+= get_R_values_w(score_obj['noun_list'], score_obj['vo'], vo, 1,'N', param, lemmatizer, score_obj['count'])\n",
    "    phi_vo+= get_R_values_w(score_obj['verb'], score_obj['vo'], vo, 0,'V', param, lemmatizer, score_obj['count'])\n",
    "  \n",
    "  # Calculating for sub\n",
    "  ind = np.random.choice(np.arange(score_obj['type_sub']), 30)\n",
    "  svo_ll = [list(score_obj['sub'].keys())[j] for j in ind]\n",
    "  for sub in svo_ll:\n",
    "    for word in score_obj['noun_list']:\n",
    "      phi_sub += get_R_p2w(sub, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "  \n",
    "  # Calculating for verb\n",
    "  ind = np.random.choice(np.arange(score_obj['type_verb']), 30)\n",
    "  svo_ll = [list(score_obj['verb'].keys())[j] for j in ind]\n",
    "  for verb in svo_ll:\n",
    "    for word in score_obj['verb_list']:\n",
    "      phi_verb += get_R_p2w(verb, word, score_obj['parameters']['R'], 'V', lemmatizer, score_obj['count'])\n",
    "\n",
    "  # Calculating for sub\n",
    "  ind = np.random.choice(np.arange(score_obj['type_obj']), 30)\n",
    "  svo_ll = [list(score_obj['obj'].keys())[j] for j in ind]\n",
    "  for obj in svo_ll:\n",
    "    for word in score_obj['noun_list']:\n",
    "      phi_obj += get_R_p2w(obj, word, score_obj['parameters']['R'], 'N', lemmatizer, score_obj['count'])\n",
    "\n",
    "  # Updating the parameters\n",
    "  score_obj['phi_svo'] = phi_svo*score_obj['type_svo']/30\n",
    "  score_obj['phi_sv'] = phi_sv*score_obj['type_sv']/30\n",
    "  score_obj['phi_vo'] = phi_vo*score_obj['type_vo']/30\n",
    "  score_obj['phi_sub'] = phi_sub*score_obj['type_sub']/30\n",
    "  score_obj['phi_verb'] = phi_verb*score_obj['type_verb']/30\n",
    "  score_obj['phi_obj'] = phi_obj*score_obj['type_obj']/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7SOVl6h9Nx2"
   },
   "outputs": [],
   "source": [
    "# Calculating sigmoid of PMI\n",
    "# !! What about P(Sub), P(Verb), P(Obj)\n",
    "# Calculate PMI or just directly use the probability as score?\n",
    "def cal_mi(sub,verb,obj, score_obj, lemmatizer):\n",
    "  prob_svo = get_prob_svo((sub,verb,obj), score_obj, lemmatizer)\n",
    "  prob_sub = get_prob_sub(sub, score_obj, lemmatizer)\n",
    "  prob_obj = get_prob_obj(obj, score_obj, lemmatizer)\n",
    "  prob_verb = get_prob_verb(verb, score_obj, lemmatizer)\n",
    "  prob_sv = get_prob_sv((sub,verb), score_obj, lemmatizer)\n",
    "  prob_vo = get_prob_vo((verb,obj), score_obj, lemmatizer)\n",
    "    \n",
    "  mi_svo = prob_svo/(prob_sub*prob_verb*prob_obj)\n",
    "  mi_sv = prob_sv/(prob_sub*prob_verb)\n",
    "  mi_vo = prob_vo/(prob_verb*prob_obj)\n",
    "  \n",
    "  return mi_svo/(1+mi_svo), mi_sv/(mi_sv+1), mi_vo/(1+mi_vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdHVCcN9D85G"
   },
   "outputs": [],
   "source": [
    "def xsigmoid(x):\n",
    "  return x / (1 + np.exp(-x))\n",
    "\n",
    "def get_final_score(sub, verb, obj, verb_t, score_obj, lemmatizer):\n",
    "  s_svo, s_sv, s_vo = cal_mi(sub,verb,obj, score_obj, lemmatizer)\n",
    "  s_svto, s_svt, s_vto = cal_mi(sub,verb_t,obj, score_obj, lemmatizer)\n",
    "\n",
    "  score_vec = np.array([s_svo, s_svto, s_sv, s_svt, s_vo, s_vto])\n",
    "  xsigmoid_score = np.dot(np.vectorize(xsigmoid)(score_vec), score_vec)\n",
    "  fxsigmoid_score = np.dot(np.vectorize(xsigmoid)(score_vec), np.multiply(score_vec , score_obj['parameters']['F']))\n",
    "  linear_score = np.dot(score_obj['parameters']['F'], score_vec)\n",
    "\n",
    "  return xsigmoid_score, linear_score, fxsigmoid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "wJzFea-V5Iwn",
    "outputId": "286664b0-c7cd-40ad-cf5b-84ddd6448390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDcjRlKhsCjn"
   },
   "outputs": [],
   "source": [
    "#score_obj['phi_svo'] = score_obj['phi_svo']*0.001\n",
    "#score_obj['phi_sv'] = score_obj['phi_sv']*0.001\n",
    "#score_obj['phi_vo'] = score_obj['phi_vo']*0.001\n",
    "#score_obj['phi_sub'] = score_obj['phi_sub']*0.001\n",
    "#score_obj['phi_verb'] = score_obj['phi_verb']*0.001\n",
    "#score_obj['phi_obj'] = score_obj['phi_obj']*0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "faZRxf_d4vZf",
    "outputId": "1f096269-3f55-4dc7-8fb4-a67dc36b918e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svo\n"
     ]
    }
   ],
   "source": [
    "set_phi_values_approx(score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gpeNVAd1ea2t",
    "outputId": "2204f976-e9a0-4074-eaac-cdf188b7f13d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.799633407929544, 2.5106065949875678, 1.501712713342899)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('you','mix','it','combine', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "365tM19X3LIk",
    "outputId": "39732ef9-2ecd-4fe9-f219-e1d1eab62fd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.3991884090499136, 2.795268817617087, 1.811958060882159)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('you','mix','it','die', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "43ZKQjYrsmC4",
    "outputId": "e5ca7075-2867-433a-d293-31e536098773"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.187166218432598, 3.048077326793292, 2.183955945715394)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('government','provide','cash','supply', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6J5ktBKDs27Y",
    "outputId": "ce66733c-854e-492b-ea2e-e18661436edd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9449043224495606, 2.9680466002144255, 2.062812300383214)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('government','provide','cash','leave', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1RYJvFwitt-C",
    "outputId": "97975946-b8c0-495e-fd75-fffb01033a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.302222563427463, 3.0791867809467925, 2.2322911136904753)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('priest','say','prayer','state', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l807moKTt6cX",
    "outputId": "17a3738b-c2d8-4636-cb2d-bd31ba6e47a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.305112085002827, 3.0800584763128827, 2.233735913495684)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_score('priest','say','prayer','allege', score_obj, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PaN62dtDoZM"
   },
   "outputs": [],
   "source": [
    "fp = open('/Datasets/TestingDatasets/GS2011data.txt', 'r')\n",
    "line = fp.readline()\n",
    "line = fp.readline()\n",
    "\n",
    "groups = {}\n",
    "\n",
    "while(line):\n",
    "    a = line.split()\n",
    "    v = a[1]\n",
    "    s = a[2]\n",
    "    o = a[3]\n",
    "    vt = a[4]\n",
    "    if (s,o) in groups:\n",
    "      if (s,v,o, vt) in groups[(s,o)]:\n",
    "        groups[(s,o)][(s,v,o, vt)]+=int(a[5])\n",
    "      else:\n",
    "        groups[(s,o)][(s,v,o, vt)]=int(a[5])\n",
    "    else:\n",
    "      groups[(s,o)] = {}\n",
    "      groups[(s,o)][(s,v,o, vt)]=int(a[5])\n",
    "    line = fp.readline()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efCZciyBGNeI"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "ZPNMV_aOG7os",
    "outputId": "5de885e6-7c5f-4b5b-efe1-54be988f362a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07216494845360823 0.07216494845360823 0.07216494845360823\n"
     ]
    }
   ],
   "source": [
    "spearman1 = 0\n",
    "spearman2 = 0\n",
    "spearman3 = 0\n",
    "counter = 0\n",
    "for so in groups:\n",
    "  model_score_1 = []\n",
    "  model_score_2 = []\n",
    "  model_score_3 = []\n",
    "  data_score = []\n",
    "  for svovt in groups[so]:\n",
    "    sub = lemmatizer.lemmatize(svovt[0])\n",
    "    obj = lemmatizer.lemmatize(svovt[2])\n",
    "    vv = lemmatizer.lemmatize(svovt[1])\n",
    "    vt = lemmatizer.lemmatize(svovt[3])\n",
    "    s = get_final_score(sub,vv,obj,vt, score_obj,lemmatizer)\n",
    "    model_score_1.append(s[0])\n",
    "    model_score_2.append(s[1])\n",
    "    model_score_3.append(s[2])\n",
    "    data_score.append(groups[so][svovt])\n",
    "  s1 = spearmanr(np.argsort(np.argsort(model_score_1)), data_score)[0]\n",
    "  s2 = spearmanr(np.argsort(np.argsort(model_score_2)), data_score)[0]\n",
    "  s3 = spearmanr(np.argsort(np.argsort(model_score_3)), data_score)[0]\n",
    "  if s1 < 2:\n",
    "    spearman1+=s1\n",
    "    counter+=1\n",
    "  if s2 < 2:\n",
    "    spearman2+=s2\n",
    "  if s3 < 2:\n",
    "    spearman3+=s3\n",
    "\n",
    "print(spearman1/counter, spearman2/counter, spearman3/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7sJQWXaG7Zc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbXcCHBihzWq"
   },
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "with open('Good_english/score_obj_extractor','wb') as f:\n",
    "  p.dump(score_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4v0b-uGWt1YJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Good_english_model_wordnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
